\section{\uppercase{3D scene modeling}}\label{sec:modeling}

\noindent For being able to compute the surface coverage of a given set of target objects that a given constellation of sensors can observe, it is necessary to model the 3D geometry of the scene objects. Moreover, the 3D sensors must have a realistic data acquisition model that is representative of the real sensors. As such, the development of the proposed system started with the 3D modeling of the scene geometry, namely the environment objects and sensors 3D models using \gls{cad} systems. Later on, several types of depth sensors were modeled within the Gazebo simulator\footnote{\url{http://gazebosim.org}} in order to perform accurate 3D rendering of the scene and generate representative sensor data taking into consideration the specific characteristics of each type of sensor (such as image resolution, field of view and depth range) while also accounting for the occlusions that other objects in the environment might cause in relation to the target models that each sensor is trying to observe from its given view point. The next sections present in detail the sensor and environment modeling along with how these sensors can be deployed in the simulation environment within plausible regions of interest that take into account where the sensors can be placed in the real environment.


\subsection{\uppercase{Environment modeling}}

Implementation text.

\begin{itemize}
	\item Modeling of 4 different test environments
	\begin{itemize}
		\item 1 for active perception, with hand occlusions
		\item 1 for bin picking with minimal occlusions
		\item 1 for bin picking with significant occlusions
		\item 1 for multiple object bin picking with several occlusions
	\end{itemize}
	\item Modeling of 3 bin picking objects
	\begin{itemize}
		\item Starter motor (target object for bin picking)
		\item Alternator (for bin picking occlusions)
		\item Differential gearbox (for bin picking occlusions)
	\end{itemize}
	\item Modeling of 2 support objects
	\begin{itemize}
		\item Trolley with shelves
		\item Large stacking box
	\end{itemize}
	\item Target objects use a special surface material that ignores light effects (such as light shading and shadows) and have a specific color (pure green) that will be used for sensor data segmentation. It was used .stl CAD models for 3D rendering and .ply point clouds for 3D data processing
\end{itemize}

\begin{figure}
	\centering
	\includegraphics[height=.126\textwidth]{environments/active-perception/gazebo-back}
	\includegraphics[height=.126\textwidth]{environments/active-perception/gazebo-top}\\
	\includegraphics[height=.126\textwidth]{environments/active-perception/rviz-back-left}
	\includegraphics[height=.126\textwidth]{environments/active-perception/rviz-back-right}
	\caption{Active perception environment renderings from Gazebo with target objects in green (top images) and associated CAD model point clouds displayed as blue spheres in Rviz (bottom images)}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[height=.126\textwidth]{environments/bin-picking/gazebo-front}
	\includegraphics[height=.126\textwidth]{environments/bin-picking/gazebo-top}\\
	\includegraphics[height=.126\textwidth]{environments/bin-picking/rviz-front}
	\includegraphics[height=.126\textwidth]{environments/bin-picking/rviz-top}
	\caption{Bin picking environment renderings from Gazebo with target objects in green (top images) and associated CAD model point clouds displayed as blue spheres in Rviz (bottom images)}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[height=.126\textwidth]{environments/bin-picking-with-occlusions/gazebo-front}
	\includegraphics[height=.126\textwidth]{environments/bin-picking-with-occlusions/gazebo-top}\\
	\includegraphics[height=.126\textwidth]{environments/bin-picking-with-occlusions/rviz-front}
	\includegraphics[height=.126\textwidth]{environments/bin-picking-with-occlusions/rviz-top}
	\caption{Bin picking with occlusions environment renderings from Gazebo with target objects in green (top images) and associated CAD model point clouds displayed as blue spheres in Rviz (bottom images)}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[height=.126\textwidth]{environments/multiple-bin-picking-with-occlusions/gazebo-front}
	\includegraphics[height=.126\textwidth]{environments/multiple-bin-picking-with-occlusions/gazebo-top}\\
	\includegraphics[height=.126\textwidth]{environments/multiple-bin-picking-with-occlusions/rviz-front-corner}
	\includegraphics[height=.126\textwidth]{environments/multiple-bin-picking-with-occlusions/rviz-top}
	\caption{Multiple bin picking with occlusions environment renderings from Gazebo with target objects in green (top images) and associated CAD model point clouds displayed as blue spheres in Rviz (bottom images)}
\end{figure}


\subsection{\uppercase{Sensors modeling}}

It was modeled 8 different types of depth sensors, 6 were structured light sensors and two were stereo vision systems.

\begin{itemize}
	\item Structured light sensors
	\begin{itemize}
		\item Asus Xtion Pro Live
		\item Ensenso N35
		\item Intel RealSense SR300
		\item Kinect XBox 360
		\item Kinect XBox One
		\item Orbbec Astra
	\end{itemize}
	\item Stereo sensors
	\begin{itemize}
		\item MultiSense S7
		\item ZED stereo camera
	\end{itemize}
\end{itemize}
Pinhole camera model coupled with OpenGL color rendering and depth buffer allows to specify the sensors:
\begin{itemize}
	\item Resolution (width and height in pixels)
	\item Field of view (vertical and horizontal FOV in radians)
	\item Minimum and maximum measurement range (near and far z-buffer clipping planes in meters)
	\item Sensor acquisition rate (number of image acquisitions per second in hertz)
\end{itemize}

\begin{figure}
	\centering
	\includegraphics[width=.47\textwidth]{sensors/sensors-gray-with-links}
	\caption{Sensors 3D CAD models with the display of the depth image coordinate frames using the ROS convention of x-y-z -> forward-left-up (horizontally from top left to bottom right: Kinect XBox 360, Asus Xtion Pro Live, Ensenso N35, Kinect XBox One, Orbbec Astra, MultiSense S7, Intel RealSense SR300, ZED stereo camera)}
\end{figure}


\subsection{\uppercase{Sensors deployment}}\label{subsec:sensors-deployment}

Implementation text.

\begin{itemize}
	\item For making the estimation of the sensor disposition computational feasible, the 3D continuous space was populated with a given set of sensors that were looking at a given point (with the sensor roll either 0ยบ or random)
	\item Several populations of sensors can be added to the world
	\begin{itemize}
		\item The 3D sensor models will be hidden at rendering time to avoid occlusion of sensor data
	\end{itemize}
	\item Each population is of a given sensor type and is deployed within a given region of interest 
	\begin{itemize}
		\item This allows to restrict the spatial distribution of the sensors, for example a given set of sensors should be in the walls or ceiling due to their weight, or they should be close to the target object given their limited depth measurements range
	\end{itemize}
	\item Currently supported deployment configurations:
	\begin{itemize}
		\item Uniform or random deployment within a box
		\item Uniform or random deployment within a cylinder
		\item Uniform within a 2D grid (with a set of rows and columns)
		\item Uniform along a line
	\end{itemize}
\end{itemize}

\begin{itemize}
	\item For the active perception environment, 450 sensors were deployed close to the target object, on the top, right and back side of the trolley
	\item This was done to simulate the closest range in which a dynamically moving sensor attached to a robotic arm could move (taking into consideration the human safety and the sensor minimum measurement distance, that was 0.2 meters)
\end{itemize}
\begin{figure}
	\centering
	\includegraphics[height=.22\textwidth]{sensor-deployment/active-perception/gazebo-front}\hspace{1em}
	\includegraphics[height=.22\textwidth]{sensor-deployment/active-perception/gazebo-top}
	\caption{Sensors deployment on the active perception environment}
\end{figure}


\begin{itemize}
	\item For the single bin picking environments, given that the target object was inside the stacking box, the sensors were deployed close to the target object, but only on top of the trolley, on 3 layers (each with a different type of sensor).
	\item In the world with minimal occlusions it was deployed 100 sensors while in the world with significant occlusions it was deployed 300 sensors
	\begin{itemize}
		\item The sensor density was increased given that the best views have tighter observation regions which could be missed with a sparse sensor deployment
	\end{itemize}
\end{itemize}
\begin{figure}
	\centering
	\includegraphics[height=.29\textwidth]{sensor-deployment/bin-picking/gazebo-sensors}\hspace{1em}
	\includegraphics[height=.29\textwidth]{sensor-deployment/bin-picking-with-occlusions/gazebo-sensors}
	\caption{Sensors deployment on the single bin picking environments}
\end{figure}

\begin{itemize}
	\item For the multiple bin picking environment, given that there were multiple target objects (1 inside the stacking box, 1 on top and 2 on the shelves of the trolley), it was deployed 450 sensors across 7 populations:
	\begin{itemize}
		\item 5 populations simulating fixed sensors on the walls and ceiling
		\item 2 populations above the trolley, simulating dynamic sensors attached to a robotic arm
	\end{itemize}
\end{itemize}
\begin{figure}
	\centering
	\includegraphics[height=.164\textwidth]{sensor-deployment/multiple-bin-picking-with-occlusions/gazebo-front}\hspace{1em}
	\includegraphics[height=.164\textwidth]{sensor-deployment/multiple-bin-picking-with-occlusions/gazebo-top}
	\caption{Sensors deployment on the multiple bin picking environment}
\end{figure}
