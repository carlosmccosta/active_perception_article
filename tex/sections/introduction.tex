\section{\uppercase{Introduction}}\label{sec:introduction}

%Problem description
%Relevance of work / motivation
%Usages
%	multiple sensor deployment for:
%		active perception
%		bin picking
%		improve confidence in object recognition
%		decide type, number and spatial distribution of sensors
%		guide dynamic sensors
%Implementation highlights
%	modelling of 4 environments and 8 sensor types within Gazebo
%	sensor deployment within rois in environments with high occlusion of target objects
%	generation of the segmented sensor point clouds
%		color segmentation
%		3d point cloud generation from depth image using the pin hole model
%		voxel grid filtering for regular space partition and fast coverage estimation
%	quick estimation of the best sensor
%	ransac approach to estimate a constellation of sensors
%Difficulties that it overcomes
%Main results
%Paper outline

\noindent Active perception within environments with large and dynamic occlusions is a challenging task that can be tackled by either deploying an extensive and expensive sensor constellation or by actively moving a set of sensors within the environment in order to maximize the observable surface area of the target objects. This is a challenging combinatorial explosive problem that has a wide range of applications within the active perception domain, such as robust object recognition for bin picking operations, object tracking for robot programming by demonstration, next best view estimation for object 3D reconstruction, among many others.

With these goals and possible applications in mind, it was developed a system for the estimation of the sensor constellation that maximizes the observable surface area of a given set of targets objects within a simulated scene with occluding geometry. The systems allows to estimate static, dynamic and hybrid constellations of sensors in simulation, making it suitable as a decision support system for selecting the type, number and spatial disposition of sensors; as a control support system for guiding moving sensors within dynamic scenes in order to avoid major occlusions and be able to keep tracking the target objects; and also as a recognition support system for improving the confidence of object detection system (given an initial estimation of the target object and the occluding objects, where should the dynamic sensor move, in order to gather additional sensor information to improve the confidence in the object recognition and increase the accuracy of the pose estimation).

The development of the proposed system was split into 4 main stages. In the first step it was modeled the 3D scene geometry of both target and occluding objects, which was necessary to create the 4 simulation environments targeting active perception and bin picking operations. Then, a set of sensor populations was deployed in each environment within regions of interest in which useful sensor data could be retrieve (given the sensors characteristics and physical constraints of the real sensors). Each population was of a specific sensor type that simulated the main hardware characteristics of commercially available sensors, such as the depth camera resolution, its field of view, range of valid measurements and acquisition rate. The third step included the generation and analysis of the sensor data for each sensor. This included the extraction of the target objects point clouds using color segmentation (target objects had a unique color material that was not affected by lighting effects), followed by the 3D projection of the 2D depth pixels using the pinhole camera model, which were later on transformed into the world coordinate system (for fast merging of data from different sensors) and filtered with a voxel grid. This filtering step was critical to ensure consistent surface area evaluation even when sensors with different image resolution where observing the same surface area at varying distances. This regular space partition assumes that too many points within a small region do not contribute to better 3D perception, and as such, a given surface cell can be considered as observed if it has at least one sensor measurement. This approach also allows to very efficiently compute the surface area coverage (by simply dividing the number of observed voxels by the number of expected surface voxels). Finally, in the forth stage, the best sensor constellation for each testing environment was estimated. If only one sensor was available, the sensor with the best surface coverage was selected. On the other hand, if several sensors could be used, it was employed a \gls{ransac} approach to estimate the N sensors that when merging and filtering their measurements managed to achieve the best surface coverage of the target objects.

In the following section it will be given a brief overview of related work developed over the years in the areas of active perception and sensor deployment. Later on, \cref{sec:modeling} will present how the 3D testing environments were created, including the sensors modeling and deployment. Then in \cref{sec:best-views-estimation} it will be introduced the algorithms used to process the sensor data and estimate the best sensor constellation, which will be supported by an experimental evaluation that will be discussed in \cref{sec:results}. Finally, \cref{sec:conclusions} will present the conclusions and give some prospects for future work.
