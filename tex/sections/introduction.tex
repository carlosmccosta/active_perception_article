\section{Introduction}\label{sec:introduction}

%Problem description
%Relevance of work / motivation
%Usages
%	multiple sensor deployment for:
%		active perception
%		bin picking
%		improve confidence in object recognition
%		decide type, number and spatial distribution of sensors
%		guide dynamic sensors
%Implementation highlights
%	modelling of 4 environments and 8 sensor types within Gazebo
%	sensor deployment within rois in environments with high occlusion of target objects
%	generation of the segmented sensor point clouds
%		color segmentation
%		3d point cloud generation from depth image using the pin hole model
%		voxel grid filtering for regular space partition and fast coverage estimation
%	quick estimation of the best sensor
%	ransac approach to estimate a constellation of sensors
%Difficulties that it overcomes
%Main results
%Paper outline

Object recognition within environments with large and dynamic occlusions is a challenging task that can be tackled by either deploying an extensive and expensive sensor constellation or by actively moving a set of sensors within the environment in order to maximize the observable surface area of the target objects. This is a variant of the View Planning Problem (VPP) \cite{Zeng2020}, which has a wide range of applications within the active perception domain, such as the estimation of the next best view for 3D scanning \cite{Mendoza2020}, object recognition with occlusions, exploration of unknown environments, deployment of sensor networks to monitor targets, among many others.

%The estimation of the best views for observing a set of target objects within the environment has received a lot of attention over the years by several branches of the scientific community, given its wide range of applications, such as active perception and exploration of the environment, 3D model scanning, sensor networks deployment, among many others.

Within the active perception domain, several approaches have been proposed depending on the particular use cases. In \cite{Eidenberger2010} it was presented a probabilistic active planner using a partially observable decision process model for improving the perception of house hold objects that were going to be manipulated by a dual arm robot. For active perception of objects with similar 3D geometry but with unique 2D features, \cite{Stampfer2012} introduced an active perception system for actively looking for bar code regions and unique text on the surface of the objects after having a preliminary estimation of the target object pose. To compute the next best sensor observation pose, it was uniformly generated a set of possible viewpoints on sections of the surface of a sphere and then it was selected the one which achieved the best observation utility that incorporates the quality of the pre-compute features along with the distance that is required to move the sensor from its current pose to the new sensor view observation pose. On the other hand, \cite{Atanasov2014} also selected the best next view by balancing the expected information gain with the required sensor movement while using a VP-tree for performing object recognition and pose estimation. Another approach introduced in \cite{Holz2014} targeted bin picking operations and relied on randomly deploying a set of possible views over the target objects and then selecting the one with the best trade off between information gain and sensor traveling cost. Unlike previous approaches that used image or geometric features, this system modeled each target object as a set of primitive shapes (such as planes, cylinders and spheres) that were assembled on a graph and recognized on the sensor data using a \gls{ransac} algorithm. Another approach presented in \cite{Potthast2014} also starts with a randomly generated set of viewpoints for the estimation of the first best view, but them reduces the regions in which views are generated and favors frontiers between known and unknown environment sections that are stored in a voxel grid. The observation probability was modeled as a hidden Markov model and the posterior probability relied on a Bayes filter. In the end, the sensor view that achieves higher information gain is selected and the world model is updated to reflect the new observed space. In \cite{Mezei2016} it is presented another system for active bin picking that takes advantage of the accurate modeling of range sensors that was presented in \cite{Gschwandtner2011}, while \cite{Hu2022} introduces strategies to generate sensor views targeted for object pose estimation.

Besides active perception, the best view estimation algorithms can also be used to actively explore the environment for mapping purposes using aerial vehicles in which the goal is to estimate the minimum set of sensor views that maximize the observation of the unknown space. The system proposed in \cite{Bircher2016} achieves this by using a receding horizon path planning algorithm while the approach presented in \cite{Gedicke2016} expands the observation goals further and tries to find a given target object in a continuously updated environment. The planning of the set of views necessary to explore the environment relied on an octree for space modeling and randomly deployed observation views along frontier regions that were later on analyzed and selected based on the expected information gain and traveling distance of the mobile robot. These exploration goals were also taken to underwater environments by the work presented in \cite{Sheinin2016}, in which special care was taken to model the sensor data degradation over distance and the necessity of artificial light for deep sea exploration and mapping.

Another research domain that uses best view estimation algorithms is 3D scanning and reverse engineering \cite{Jubert2021}. In \cite{Irving2014} it is introduced a volumetric 3D modeling scanning system that deploys a set of possible viewpoints on a tessellated sphere or a cube and then based on the expected information gain, the overlap of known and unknown regions, distance to previous selected view and orientation of the view to the observed surface, it selects the set of sensor views required to perform 3D scanning and surface reconstruction of the object.
In \cite{Krainin2011}, besides actively moving the sensor, it is used a robotic manipulator for grasping an object and then estimate the constellation of sensor views that maximize the amount of unknown object cells that can be observed while keeping a reasonable overlap with know regions. After finishing a set of observations, the system chooses another grasp configuration and tries to observe the remaining regions that were previously occluded by the robotic arm.

A related area to best view estimation is sensor deployment for monitoring extensive areas in order to track a set of interest objects or providing a communication infrastructure. In \cite{Guo2008} it is introduced an adaptive 2D sensor placement and boundary estimation system for monitoring and tracking objects. The disposition of the sensors is based on signal propagation and area coverage and aims to track (with the minimum number of sensors) a given set of objects modeled as Gaussian mixture of models that are updated using a recursive distributive expectation maximization algorithm. Extending the sensor deployment to 3D, \cite{Zhao2013} provides an optimal range sensor placement approach for minimizing the target localization uncertainty using the Fisher information matrix.%, that is modeled as the inverse of the Cramer Rao lower bound.% On the other hand, \cite{Yuan2008} provides a divide and conquer approach for monitoring static objects while \cite{Salinas2013} provides an analytical formulation for maximizing the Fisher information matrix for tracking a set of targets in 2D.

With these goals and possible applications in mind, it was developed a system\footnote{\url{https://github.com/carlosmccosta/sensor_placement_optimization}} for the estimation of the sensor constellation that maximizes the observable surface area (cost function) of a given set of target objects within a simulated scene with occluding geometry. By relying on simulated sensors and environments, the system allows quick evaluation of which types of sensors and which environment regions maximize the capture of 3D data for achieving better surface coverage of the target objects, making it suitable as a decision support system for helping the deployment of sensor constellations.

% Moreover, it can also be used as a control support system for guiding moving sensors within dynamic scenes in order to avoid major occlusions and be able to keep tracking the target objects. On the other hand, it may be employed as a recognition support system for improving the confidence of object detection systems, since it can indicate where should the dynamic sensor move in order to increase the observable surface area of the target objects for improving the confidence in the object recognition and increase the accuracy of the pose estimation.

The development of the proposed system was split into 4 main stages. In the first step it was modeled the 3D scene geometry of both the target and occluding objects. For testing the capabilities of the system, 4 simulation environments were created, targeting active perception and bin picking applications. Then, a set of sensor populations was deployed in each environment within regions of interest in which useful sensor data could be retrieve (given the sensors characteristics and physical constraints of the real sensors). Each population was of a specific sensor type that simulated the main hardware characteristics of commercially available sensors, such as the depth camera resolution, its field of view, range of valid measurements and acquisition rate. The third step included the generation and analysis of the sensor data for each sensor. This included the extraction of the target objects point clouds using color segmentation (the target objects had a unique color material that was not affected by lighting effects), followed by the 3D projection of the 2D depth pixels using the pinhole camera model, which were later on transformed into the world coordinate system (for fast merging of data from different sensors) and filtered with a voxel grid. This filtering step was critical to ensure consistent surface area evaluation even when sensors with different image resolution where observing the same surface area at varying distances. This regular space partition assumes that too many points within a small region do not contribute to better 3D perception, and as such, a given surface cell can be considered as observed if it has at least one sensor measurement. This approach also allows to very efficiently compute the surface area coverage (by simply dividing the number of observed voxels by the number of expected surface voxels). Finally, in the forth stage, the best sensor constellation for each testing environment was estimated. When the goal was the selection of a single sensor, then the simulated sensor with the best surface coverage was selected. On the other hand, if several sensors could be used, a \gls{ransac} approach was employed to estimate the N sensors that when merging and filtering their measurements managed to achieve the best surface coverage of the target objects.

The main contributions of the paper are the proposal of an efficient sensor fusion method that relies on color segmentation of the target objects followed by voxel grid merging, along with a \gls{ransac} approach that computes a constellation of sensors that maximizes the surface area coverage of a set of target objects deployed in simulated environments. Moreover, for allowing future benchmarking of systems with similar goals, the testing environments and the implementation were made publicly available.

In the following section it will be presented how the 3D testing environments were created, including the sensors modeling and deployment. Then in \cref{sec:best-views-estimation} it will be introduced the algorithms used to process the sensor data and estimate the best sensor constellation, which will be supported by an experimental evaluation that will be discussed in \cref{sec:results}. Finally, \cref{sec:conclusions} will present the conclusions.
