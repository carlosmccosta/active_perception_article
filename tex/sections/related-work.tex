\section{Related Work}\label{sec:related-work}

The estimation of the best views for observing a set of target objects within the environment has received a lot of attention over the years by several branches of the scientific community, given its wide range of applications, such as active perception and exploration of the environment, 3D model scanning, sensor networks deployment, among many others.

Within the active perception domain, several approaches have been proposed depending on the particular use cases. In \cite{Eidenberger2010} it was presented a probabilistic active planner using a partially observable decision process model for improving the perception of house hold objects that were going to be manipulated by a dual arm robot. For active perception of objects with similar 3D geometry but with unique 2D features, \cite{Stampfer2012} introduced an active perception system for actively looking for bar code regions and unique text on the surface of the objects after having a preliminary estimation of the target object pose. To compute the next best sensor observation pose, it was uniformly generated a set of possible viewpoints on sections of the surface of a sphere and then it was selected the one which achieved the best observation utility that incorporates the quality of the pre-compute features (ability to uniquely describe the object) along with the distance that is required to move the sensor from its current pose to the new sensor view observation pose. On the other hand, \cite{Atanasov2014} also selected the best next view by balancing the expected information gain with the required sensor movement while using a VP-tree for performing object recognition and pose estimation. Another approach introduced in \cite{Nieuwenhuisen2013,Holz2014} targeted bin picking operations and relied on randomly deploying a set of possible views over the target objects and then selecting the one with the best trade off between information gain and sensor traveling cost. Unlike previous approaches that used image or geometric features, this system modeled each target object as a set of primitive shapes (such as planes, cylinders and spheres) that were assembled on a graph and recognized on the sensor data using a \gls{ransac} algorithm. Another approach presented in \cite{Potthast2014} also starts with a randomly generated set of viewpoints for the estimation of the first best view, but them reduces the regions in which views are generated and favors frontiers between known and unknown environment sections that are stored in a voxel grid. The observation probability was modeled as a hidden Markov model and the posterior probability relied on a Bayes filter. In the end, the sensor view that achieves higher information gain is selected and the world model is updated to reflect the new observed space.% In \cite{Mezei2016} it is presented another system for active bin picking that takes advantage of the accurate modeling of range sensors that was presented in \cite{Gschwandtner2011}.

Besides active perception, the best view estimation algorithms can also be used to actively explore the environment for mapping purposes using aerial vehicles in which the goal is to estimate the minimum set of sensor views that maximize the observation of the unknown space. The system proposed in \cite{Bircher2016} achieves this by using a receding horizon path planning algorithm while the approach presented in \cite{Gedicke2016} expands the observation goals further and tries to find a given target object in a continuously updated environment. The planning of the set of views necessary to explore the environment relied on an octree for space modeling and randomly deployed observation views along frontier regions that were later on analyzed and selected based on the expected information gain and traveling distance of the mobile robot. These exploration goals were also taken to underwater environments by the work presented in \cite{Sheinin2016}, in which special care was taken to model the sensor data degradation over distance and the necessity of artificial light for deep sea exploration and mapping.

Another research domain that uses best view estimation algorithms is 3D model scanning. In \cite{Irving2014} it is introduced a volumetric 3D modeling scanning system that deploys a set of possible viewpoints on a tessellated sphere or a cube and then based on the expected information gain, the overlap of known and unknown regions, distance to previous selected view and orientation of the view to the observed surface, it selects the set of sensor views required to perform 3D scanning and surface reconstruction of the object.
In \cite{Krainin2011} besides actively moving the sensor, it is used a robotic manipulator for grasping a given object and then estimate the constellation of sensor views that maximize the amount of unknown object cells that can be observed while keeping a reasonable overlap with know regions (in order to use surface matching algorithms). After finishing a set of observations, the system chooses another grasp configuration and tries to observe the remaining regions that were previously occluded by the robotic arm.

A related area to best view estimation is sensor deployment for monitoring extensive areas in order to track a given set of interest objects or providing a communication infrastructure. In \cite{Guo2008} it is introduced an adaptive 2D sensor placement and boundary estimation system for monitoring and tracking objects. The disposition of the sensors is based on signal propagation and area coverage and aims to track (with the minimum number of sensors) a given set of objects modeled as Gaussian mixture of models that are updated using a recursive distributive expectation maximization algorithm. Extending the sensor deployment to 3D, \cite{Zhao2013} provides an optimal range sensor placement approach for minimizing the target localization uncertainty using the Fisher information matrix, that is modeled as the inverse of the Cramer Rao lower bound. On the other hand, \cite{Yuan2008} provides a divide and conquer approach for monitoring static objects while \cite{Salinas2013} provides an analytical formulation for maximizing the Fisher information matrix for tracking a set of targets in 2D.

The proposed system introduces a best views estimation approach that accurately simulates 3D depth sensors, allows to customize the regions in which sensor views can be deployed and handles sensor data fusion efficiently. In the future it might be coupled with a hand tracking system \cite{Kyriazis2013,Sridhar2016} for further improving its active perception capabilities even when they are subjected to significant occlusions.
